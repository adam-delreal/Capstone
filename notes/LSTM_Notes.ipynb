{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks: Long Short Term Memory (LSTM)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks do not make any forecasts; instead, they analyze price data and uncover opportunities. \n",
    "\n",
    "- Using a neural network, you can make a trade decision based on thoroughly analyzed data, which is not necessarily the case when using traditional technical analysis methods. \n",
    "\n",
    "- Detect subtle non-linear interdependencies and patterns that other methods of technical analysis are unable to uncover."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Key Words and Recap:\n",
    "\n",
    "- **Loss Function:** A loss function Loss(x, y, w) quantifies how unhappy you would be if you used w to make a prediction on x when the correct output is y. It is the object we want to minimize. \n",
    "\n",
    "- **Gradient descent** is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.\n",
    "    -  Mathematically, a gradient is a partial derivative with respect to its inputs.\n",
    "\n",
    "    - is used to update the weights and biases to minimize this loss function. \n",
    "\n",
    "- **Backpropagation** is gradient descent applied to a neural network, where the errors are distributed _backwards_ through the model to update the weights and biases of each layer. \n",
    "\n",
    "- **Forward propagation** simply refers to the process of passing data forward through the model to generate predictions. \n",
    "\n",
    "- An **epoch** is a gradient descent iteration for the entire training set (this may include several distinct FP and BP steps if using batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Nets:** are a type of artificial neural network designed to recognize patterns in sequences of data.\n",
    "\n",
    "------ \n",
    "\n",
    "## Feed-forward Networks Recap\n",
    "\n",
    "- One feeds information straight through (never touching a given node twice), while the other cycles it through a loop, and the latter are called recurrent.\n",
    "\n",
    "- Supervised learning, the output would be a label, a name applied to the input. That is, they map raw data to categories, recognizing patterns that may signal, for example, that an input image should be labeled “orange” or “apples.”\n",
    "\n",
    "- A feedforward network is trained on labeled images until it minimizes the error it makes when guessing their categories. With the trained set of parameters (or weights, collectively known as a model), the network sallies forth to categorize data it has never seen.\n",
    "\n",
    "- a feedforward network has no notion of order in time\n",
    "\n",
    "- Feedforward networks are amnesiacs regarding their recent past; they remember nostalgically only the formative moments of training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Recurrent Networks\n",
    "\n",
    "- Recurrent networks, on the other hand, take as their input not just the current input example they see, but also what they have perceived previously in time.\n",
    "\n",
    "- The decision a recurrent net reached at time step t-1 affects the decision it will reach one moment later at time step t. So recurrent networks have two sources of input, the present and the recent past, which combine to determine how they respond to new data, much as we do in life.\n",
    "\n",
    "- It is often said that recurrent networks have memory. Adding memory to neural networks has a purpose: There is information in the sequence itself, and recurrent nets use it to perform tasks that feedforward networks can’t.\n",
    "\n",
    "- That sequential information is preserved in the recurrent network’s hidden state, which manages to span many time steps as it cascades forward to affect the processing of each new example. \n",
    "\n",
    "- It is finding correlations between events separated by many moments, and these correlations are called “long-term dependencies”, because an event downstream in time depends upon, and is a function of, one or more events that came before. One way to think about RNNs is this: they are a way to share weights over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Back-Propagation Through Time (BPTT):\n",
    "\n",
    "- Remember, the purpose of recurrent nets is to accurately classify sequential input relying on the back-propagation of error and gradient descent.\n",
    "\n",
    "- Backpropagation in feedforward networks moves backward from the final error through the outputs, weights and inputs of each hidden layer, assigning those weights responsibility for a portion of the error by calculating their partial derivatives – ∂E/∂w, or the relationship between their rates of change. Those derivatives are then used by our learning rule, gradient descent, to adjust the weights up or down, whichever direction decreases error.\n",
    "\n",
    "- Recurrent networks rely on an extension of backpropagation called backpropagation through time, or BPTT. Time, in this case, is simply expressed by a well-defined, ordered series of calculations linking one time step to the next, which is all backpropagation needs to work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "## Long Short-Term Memory Units (LSTM)\n",
    "\n",
    "- Suggested as a solution to the vanishing gradient problem.\n",
    "\n",
    "- LSTMs help preserve the error that can be backpropagated through time and layers. By maintaining a more constant error, they allow recurrent nets to continue to learn over many time steps (over 1000), thereby opening a channel to link causes and effects remotely.\n",
    "\n",
    "- LSTMs contain information outside the normal flow of the recurrent network in a gated cell. Information can be stored in, written to, or read from a cell, much like data in a computer’s memory. The cell makes decisions about what to store, and when to allow reads, writes and erasures, via gates that open and close. Unlike the digital storage on computers, however, these gates are analog, implemented with element-wise multiplication by sigmoids, which are all in the range of 0-1. Analog has the advantage over digital of being differentiable, and therefore suitable for backpropagation.\n",
    "\n",
    "- Those gates act on the signals they receive, and similar to the neural network’s nodes, they block or pass on information based on its strength and import, which they filter with their own sets of weights. Those weights, like the weights that modulate input and hidden states, are adjusted via the recurrent networks learning process. That is, the cells learn when to allow data to enter, leave or be deleted through the iterative process of making guesses, backpropagating error, and adjusting weights via gradient descent.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
